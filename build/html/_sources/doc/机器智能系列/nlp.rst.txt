============
自然语言理解
============
.. note:: 

    更新日期：2020年3月28日

书籍推荐
==========

1. `《Speech and Language Processing, 2nd Edition》 <https://book.douban.com/subject/2403834/>`_
    - 内容全面，覆盖面广
2. 《统计自然语言处理》 宗成庆
3. 《信息检索导论》
4. 《语言本能-人类语言进化的奥秘》

什么是 自然语言 理解？
======================
自然语言？
----------
1. 和编程语言相对的语言称为自然语言。

自然语言理解？
----------------

1. 目的：使计算机理解人类的自然语言。
2. 本质：结构预测的过程。如输出一句话的句法结构或者语义结构。
3. 从无结构化序列预测有结构的语义。
    - 词性标注、命名实体识别、依存分析、句法分析、指代消解等任务。
4. 数据驱动的自然语言理解
    - 深度学习技术的突破
5. 语义表示（核心）
    - one-hot （0/1）也称独热编码
        + 表示简单，但局限性很大，如相似度计算。
    - 分布式语义表示 （空间表示）
        + 目前最主要的表示形式。
        + 1986年提出的方法。

自然语言的特点
--------------

1. 歧义性多。
    ```
    南京市/长江大桥。南京市长/江大桥。
    ```
    - 关键目标：消除歧义性。
2. 递归性
    ```
    你好/不好意思。 你/好不好意思。
    ```
3. 主观性
    ```
    仔细体会这两句：别回了。 我没事，你忙你的。
    ```
4. 社会性
    - 敬语、语言协调等

难点
---------

1. 语言的语义表示
    - 世界（社会/客观）、心智（人/主观）、语言三者相互影响。
2. 将人类的知识融入到语义表示的过程中。
    - 人类知识相当于给足了上下文信息
3. 多模态复杂语境的理解 (说话的表情、手势动作、场景等)
    - 欢迎/新老/师生/前来/参观！
    - 欢迎/新老师/生前/来参观！

未来
-----------

1. 句子消歧。
2. 引入知识。如知识图谱。
3. 多级的跨句子建模。
4. 生成句子更符合当下对话场景。
5. 理解并创作。

自然语言理解交叉科学
----------------------

1. 计算机科学
2. 脑科学
3. 语言学
4. 语言哲学
5. 心理学、社会学、认知学
6. 神经语言学、汉语言学

自然语言技术方向
====================

- 基于规则驱动
- 基于数据驱动
    - 统计学语言模型
    - 深度语言模型

NLP国内外优秀学者及实验室
==============================
- `Christopher Manning <https://nlp.stanford.edu/~manning/>`_

========================================
CS224n-2019-课程笔记 by Chris Manning
========================================

.. tip::

    - `CS224n-2019 <https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/>`_.课程源自斯坦福CS course，2019年发布的自然语言处理，算是NLP的经典吧，老爷子讲的也很风趣幽默。Ok, Hello, everyone!一起来追剧吧。
    - 题外话：课时并不多，所以暗示自己要尽量耐心地把每一节的知识搞懂（慢工出细活，理论知识很重要），自己思考的同时也要动手推导公式甚至编写代码（我就是这么弄得）来刺激大脑理解，难受一阵会发现知识理解很深刻，再回顾此前的知识，豁然开朗！

一些说明和资源
==============
- `课程主页 <http://web.stanford.edu/class/cs224n/index.html>`_
- 本笔记涉及代码部分将使用pipenv构建虚拟环境。（推荐）
- 课后作业使用PyTocrh框架使用。
    + HW1-HW5.
    + FP,which is QA .
- `B站资源 <https://www.bilibili.com/video/BV1Eb411H7Pq/?spm_id_from=333.788.videocard.0)>`_
- `笔记参考 <https://looperxx.github.io/CS224n-2019-01-Introduction%20and%20Word%20Vectors/>`_

1-词向量（Word Vectors)
=========================
.. image:: ./images/nlp/preface.jpg 

自然语言和词义
---------------
1. 自然语言
    - 你永远无法确定任何单词对他人意味着什么。（中文这个情况就更普遍啦）
    - 写作是另一件让人类变得强大的事情，这实现了知识的传播和共享。
2. 语言的意义
    - 通过一个词或句子等来表达概念
    - 人们通过文字或声音信号等来表达思想、想法
    - 在写作、艺术中表达含义

::

    一般通过下面这种语言方式进行有意义的思考:
        signifier(symbol)⇔signified(idea or thing) =denotational semantics 

3. 语义计算
    - 常见方案的不足
        + 类似 `WordNet <https://wordnet.princeton.edu/)>`_ 一个面向语义的英语词典，包含上义词（hypernyms）、同义词（synonym sets）。
            - 没有考虑上下文，忽略一个词的细微差别
            - 不能及时更新。
            - Can’t compute accurate word similarity
        + 传统NLP的做法。离散符号表示。one-hot，0-1进行编码：Means one 1, the rest 0s
            - 向量大小就是词汇表的大小（很多无用的信息）
            - 无法计算相似度。如下例两个词向量是正交的，点积为0.
            
            ::

                motel = [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]
                hotel = [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]

    - 提取新方案
        + Could try to rely on WordNet’s list of synonyms to get similarity?
        + learn to encode similarity in the vectors themselves（学习词自身的编码信息）
4. 通过上下文表示词
    - 分布式语义：一个词的含义往往是由附近高频出现的词决定的。
    - word出现在文本中，这个Word周围会有由词的集合组成的Context出现。这个上下文是固定一个窗口size的。
    - 我们可以使用存在Word的大量 `语料 <http://ling.cuc.edu.cn/RawPub/)>`_ 来学习其向量表示。比如学习“中国科学院”词（实际中会学习每个词），在下列的语料中。
            1. 先向获得2009年度国家最高科学技术奖的 **中国科学院** 院士、复旦大学数学研究所名誉所长谷超豪和
            2. 院士、复旦大学数学研究所名誉所长谷超豪和 **中国科学院** 院士、中国航天科技集团公司高级技术顾
            3. 大国”向“造船强国”迈进。 由 **中国科学院** 和上海市政府共同建设的上海同步辐射光源工
            4. 丽；河南卓越工程管理有限公司董事长邬敏 **中国科学院** 研究生院教授杨佳十人“全国三八红旗手

Word Vectors(词向量)
-------------------------
- 根据一个的词的上下文，来为词构建密集的向量，以使得该向量与出现在类似上下文中的词相似
- 引出词向量，也称词嵌入或词表示。
    + word vectors are sometimes called **word embeddings** or word representations. 
    + They are a **distributed representation**.
    + 例如“中国”这个词经过训练后的词向量为：

.. math::
    中国 = \begin{pmatrix} 0.286\\
                0.792\\
                −0.177\\
                −0.107\\
                0.109\\
                −0.542\\
                0.349\\
                0.271 \end{pmatrix}

Word2Vector介绍
-------------------------
.. note::
    Word2vec (Mikolov et al. 2013) 是一种学习词向量的 *框架*。

主要思想
^^^^^^^^^
1. 我们有个比较大的文本数据集。
2. 文本中的每个词通过一个固定长度的词向量表示。
3. 扫描文本中每一个位置 **t** 所表示的词,其中有一个中心词 **c** 和上下文词 **o** 。
4. 通过c和o的词向量的相似性，计算在给定c,即中心词来计算o,即上下文的概率。反之亦然。
5. 不断调整词向量来最大化上面提到的概率。

- 举例如下

    .. image:: ./images/nlp/w2v_ex-1.png

    .. image:: ./images/nlp/w2v_ex-2.png

Word2Vector目标函数
-----------------------------
1. 思路(后面要说的Skip-grams模型）

    在每个位置 :math:`t` （t = 1，……，T)，给定一个中心词 :math:`w_j` 和一段固定长度的窗口 :math:`m`，预测上下文中每个单词的概率。

.. math::
    Likelihood = L(\theta) = \prod_{t=1}^{T}\prod _{\substack{-m\leq j \leq m \\ j\neq 0}}P(w_{t+j}|w_t;\theta)
    
    其中 \theta 是一个需要全局优化的变量

- 目标函数 :math:`J(\theta)` （也称为 **代价或损失函数**）,是一个负向对数似然：

    .. math::
        J(\theta) = -\frac{1}{T}logL(\theta) = -\frac{1}{T}\sum_{t=1}^{T}\sum _{\substack{-m\leq j \leq m \\ j\neq 0}}P(w_{t+j}|w_t;\theta)

.. tip::
    - 利用对数的特性将目标函数转换为对数求和，减少计算的复杂度。
    - 最小化目标函数 ⟺最大化预测的准确率

- 通过不断的优化参数最小化误差来训练模型。
- 为了训练模型，需要计算所有向量的梯度
    + :math:`\theta` 用一个很长的向量表示所有模型的参数。
    + 每个单词有个两个向量。
    + 利用不断移动的梯度来优化这些模型的参数。

梯度计算推导
-------------------------
.. todo::
    - 推导细节描述
    - 一些边角知识的回顾：向量求导、链式法则
    - word2vector的概览

优化：梯度下降的要点
-----------------------

小结-1
-------------------------




