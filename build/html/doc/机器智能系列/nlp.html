

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="zh-CN" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="zh-CN" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>4. 自然语言理解 &mdash; Hang&#39;s Tec Room 1.0 文档</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript">
          var DOCUMENTATION_OPTIONS = {
              URL_ROOT:'../../',
              VERSION:'1.0',
              LANGUAGE:'zh_CN',
              COLLAPSE_INDEX:false,
              FILE_SUFFIX:'.html',
              HAS_SOURCE:  true,
              SOURCELINK_SUFFIX: '.txt'
          };
      </script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../_static/translations.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="index" title="索引" href="../../genindex.html" />
    <link rel="search" title="搜索" href="../../search.html" />
    <link rel="next" title="6. 知识图谱" href="kg.html" />
    <link rel="prev" title="3. 深度学习" href="dl.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> Hang's Tec Room
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">关于Room</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../关于/about_me.html">关于我</a></li>
<li class="toctree-l1"><a class="reference internal" href="../关于/about_blog.html">我的技术屋</a></li>
</ul>
<p class="caption"><span class="caption-text">编程语言及开发框架</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../编程语言及开发框架/C++.html">1. C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="../编程语言及开发框架/Java技术栈/Java.html">2. Java</a></li>
<li class="toctree-l1"><a class="reference internal" href="../编程语言及开发框架/Java技术栈/Spring.html">3. Spring</a></li>
<li class="toctree-l1"><a class="reference internal" href="../编程语言及开发框架/Java技术栈/SpringBoot.html">4. SpringBoot基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="../编程语言及开发框架/Java技术栈/SpringBoot.html#id6">5. SpringBoot进阶</a></li>
<li class="toctree-l1"><a class="reference internal" href="../编程语言及开发框架/python技术栈/Python.html">6. python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../编程语言及开发框架/python技术栈/django.html">7. Django</a></li>
<li class="toctree-l1"><a class="reference internal" href="../编程语言及开发框架/Matlab.html">8. Matlab</a></li>
</ul>
<p class="caption"><span class="caption-text">后端接口开发</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../接口开发/接口开发.html">1. 接口（Interface）开发</a></li>
</ul>
<p class="caption"><span class="caption-text">项目开发杂谈</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../项目开发杂谈/项目开发杂谈.html">1. 项目开发杂谈</a></li>
</ul>
<p class="caption"><span class="caption-text">数据结构与算法系列</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../数据结构与算法系列/dataStruct_algorithm.html">1. 数据结构与算法系列</a></li>
</ul>
<p class="caption"><span class="caption-text">设计模式</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../设计模式/design-pattern.html">1. 设计模式</a></li>
</ul>
<p class="caption"><span class="caption-text">计算机网络技术</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../计算机网络技术/computer-network.html">1. 计算机网络系列</a></li>
</ul>
<p class="caption"><span class="caption-text">数据库系列</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../数据库系列/mysql.html">1. MySQL</a></li>
</ul>
<p class="caption"><span class="caption-text">虚拟化与容器技术</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../虚拟化与容器技术/docker.html">1. Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../虚拟化与容器技术/k8s.html">2. Kubernetes（K8S）</a></li>
</ul>
<p class="caption"><span class="caption-text">服务器</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../服务器/server.html">1. 服务器系列</a></li>
</ul>
<p class="caption"><span class="caption-text">分布式与微服务系列</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../分布式与微服务系列/distributeMicroServer.html">1. 分布式与微服务系列</a></li>
</ul>
<p class="caption"><span class="caption-text">机器智能系列</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="ml.html">1. 机器学习基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml.html#id3">2. 机器学习应用</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl.html">3. 深度学习</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">4. 自然语言理解</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id2">4.1. 书籍推荐</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id3">4.2. 什么是 自然语言 理解？</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id4">4.2.1. 自然语言？</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id5">4.2.2. 自然语言理解？</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id6">4.2.3. 自然语言的特点</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id7">4.2.4. 难点</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id8">4.2.5. 未来</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id9">4.2.6. 自然语言理解交叉科学</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id10">4.3. 自然语言技术方向</a></li>
<li class="toctree-l2"><a class="reference internal" href="#nlp">4.4. NLP国内外优秀学者及实验室</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#cs224n-2019-by-chris-manning">5. CS224n-2019-课程笔记 by Chris Manning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id11">5.1. 一些说明和资源</a></li>
<li class="toctree-l2"><a class="reference internal" href="#word-vectors">5.2. 词向量（Word Vectors)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id15">5.2.1. 自然语言和词义</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id17">5.2.2. Word Vectors(词向量)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#word2vector">5.2.3. Word2Vector介绍</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id18">5.2.3.1. 主要思想</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id19">5.2.4. Word2Vector目标函数</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id20">5.2.5. 梯度计算推导</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id21">5.2.6. word2vector的概览</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id22">5.2.7. 优化：梯度下降与随机梯度下降算法的要点</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id23">5.2.8. 小结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id24">5.3. 词向量和语义</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#review-word2vec">5.3.1. Review：word2vec</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id25">5.3.1.1. 主要思想</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id26">5.3.1.2. 关于梯度计算</a></li>
<li class="toctree-l4"><a class="reference internal" href="#negative-sample">5.3.1.3. 基于负采样(negative sample)方法计算</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id27">5.3.2. 基于共现矩阵生成词向量</a></li>
<li class="toctree-l3"><a class="reference internal" href="#glove">5.3.3. Glove词向量模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id28">5.3.4. 词向量评估</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id29">5.3.5. 语义</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id30">5.3.6. 分类和神经网络的区别</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id31">5.3.7. 建议</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="kg.html">6. 知识图谱</a></li>
<li class="toctree-l1"><a class="reference internal" href="框架/pytorch.html">7. Pytorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="框架/tensorflow.html">8. Tensorflow</a></li>
</ul>
<p class="caption"><span class="caption-text">开源工具</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../开源工具/open_source.html">1. 优秀开源工具</a></li>
</ul>
<p class="caption"><span class="caption-text">团队与项目管理</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../团队与项目管理/team_project.html">1. 团队与项目管理</a></li>
</ul>
<p class="caption"><span class="caption-text">道与术</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../道与术/tao-art.html">1. coding的道与术</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Hang's Tec Room</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
      <li>4. 自然语言理解</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/doc/机器智能系列/nlp.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="id1">
<h1>4. 自然语言理解<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h1>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">更新日期：2020年04月01日</p>
</div>
<div class="section" id="id2">
<h2>4.1. 书籍推荐<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h2>
<ol class="arabic simple">
<li><dl class="first docutils">
<dt><a class="reference external" href="https://book.douban.com/subject/2403834/">《Speech and Language Processing, 2nd Edition》</a></dt>
<dd><ul class="first last">
<li>内容全面，覆盖面广</li>
</ul>
</dd>
</dl>
</li>
<li>《统计自然语言处理》 宗成庆</li>
<li>《信息检索导论》</li>
<li>《语言本能-人类语言进化的奥秘》</li>
</ol>
</div>
<div class="section" id="id3">
<h2>4.2. 什么是 自然语言 理解？<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h2>
<div class="section" id="id4">
<h3>4.2.1. 自然语言？<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h3>
<ol class="arabic simple">
<li>和编程语言相对的语言称为自然语言。</li>
</ol>
</div>
<div class="section" id="id5">
<h3>4.2.2. 自然语言理解？<a class="headerlink" href="#id5" title="永久链接至标题">¶</a></h3>
<ol class="arabic simple">
<li>目的：使计算机理解人类的自然语言。</li>
<li>本质：结构预测的过程。如输出一句话的句法结构或者语义结构。</li>
<li><dl class="first docutils">
<dt>从无结构化序列预测有结构的语义。</dt>
<dd><ul class="first last">
<li>词性标注、命名实体识别、依存分析、句法分析、指代消解等任务。</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>数据驱动的自然语言理解</dt>
<dd><ul class="first last">
<li>深度学习技术的突破</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>语义表示（核心）</dt>
<dd><ul class="first last">
<li><dl class="first docutils">
<dt>one-hot （0/1）也称独热编码</dt>
<dd><ul class="first last">
<li>表示简单，但局限性很大，如相似度计算。</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>分布式语义表示 （空间表示）</dt>
<dd><ul class="first last">
<li>目前最主要的表示形式。</li>
<li>1986年提出的方法。</li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ol>
</div>
<div class="section" id="id6">
<h3>4.2.3. 自然语言的特点<a class="headerlink" href="#id6" title="永久链接至标题">¶</a></h3>
<ol class="arabic simple">
<li><dl class="first docutils">
<dt>歧义性多。</dt>
<dd><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">南京市/长江大桥。南京市长/江大桥。</span>
<span class="pre">`</span></code>
- 关键目标：消除歧义性。</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>递归性</dt>
<dd><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">你好/不好意思。</span> <span class="pre">你/好不好意思。</span>
<span class="pre">`</span></code></dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>主观性</dt>
<dd><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">仔细体会这两句：别回了。</span> <span class="pre">我没事，你忙你的。</span>
<span class="pre">`</span></code></dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>社会性</dt>
<dd><ul class="first last">
<li>敬语、语言协调等</li>
</ul>
</dd>
</dl>
</li>
</ol>
</div>
<div class="section" id="id7">
<h3>4.2.4. 难点<a class="headerlink" href="#id7" title="永久链接至标题">¶</a></h3>
<ol class="arabic simple">
<li><dl class="first docutils">
<dt>语言的语义表示</dt>
<dd><ul class="first last">
<li>世界（社会/客观）、心智（人/主观）、语言三者相互影响。</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>将人类的知识融入到语义表示的过程中。</dt>
<dd><ul class="first last">
<li>人类知识相当于给足了上下文信息</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>多模态复杂语境的理解 (说话的表情、手势动作、场景等)</dt>
<dd><ul class="first last">
<li>欢迎/新老/师生/前来/参观！</li>
<li>欢迎/新老师/生前/来参观！</li>
</ul>
</dd>
</dl>
</li>
</ol>
</div>
<div class="section" id="id8">
<h3>4.2.5. 未来<a class="headerlink" href="#id8" title="永久链接至标题">¶</a></h3>
<ol class="arabic simple">
<li>句子消歧。</li>
<li>引入知识。如知识图谱。</li>
<li>多级的跨句子建模。</li>
<li>生成句子更符合当下对话场景。</li>
<li>理解并创作。</li>
</ol>
</div>
<div class="section" id="id9">
<h3>4.2.6. 自然语言理解交叉科学<a class="headerlink" href="#id9" title="永久链接至标题">¶</a></h3>
<ol class="arabic simple">
<li>计算机科学</li>
<li>脑科学</li>
<li>语言学</li>
<li>语言哲学</li>
<li>心理学、社会学、认知学</li>
<li>神经语言学、汉语言学</li>
</ol>
</div>
</div>
<div class="section" id="id10">
<h2>4.3. 自然语言技术方向<a class="headerlink" href="#id10" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li>基于规则驱动</li>
<li><dl class="first docutils">
<dt>基于数据驱动</dt>
<dd><ul class="first last">
<li>统计学语言模型</li>
<li>深度语言模型</li>
</ul>
</dd>
</dl>
</li>
</ul>
</div>
<div class="section" id="nlp">
<h2>4.4. NLP国内外优秀学者及实验室<a class="headerlink" href="#nlp" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li><a class="reference external" href="https://nlp.stanford.edu/~manning/">Christopher Manning</a></li>
</ul>
</div>
</div>
<div class="section" id="cs224n-2019-by-chris-manning">
<h1>5. CS224n-2019-课程笔记 by Chris Manning<a class="headerlink" href="#cs224n-2019-by-chris-manning" title="永久链接至标题">¶</a></h1>
<div class="admonition tip">
<p class="first admonition-title">小技巧</p>
<ul class="last simple">
<li><a class="reference external" href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/">CS224n-2019</a>.课程源自斯坦福CS course，2019年发布的自然语言处理，算是NLP的经典吧，老爷子讲的也很风趣幽默。Ok, Hello, everyone!一起来追剧吧。</li>
<li>题外话：课时并不多，所以暗示自己要尽量耐心地把每一节的知识搞懂（慢工出细活，理论知识很重要），自己思考的同时也要动手推导公式甚至编写代码（我就是这么弄得）来刺激大脑理解，难受一阵会发现知识理解很深刻，再回顾此前的知识，豁然开朗！</li>
</ul>
</div>
<div class="section" id="id11">
<h2>5.1. 一些说明和资源<a class="headerlink" href="#id11" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li><a class="reference external" href="http://web.stanford.edu/class/cs224n/index.html">课程主页</a></li>
<li>本笔记涉及代码部分将使用pipenv构建虚拟环境。（推荐）</li>
<li><dl class="first docutils">
<dt>课后作业使用PyTocrh框架使用。</dt>
<dd><ul class="first last">
<li>HW1-HW5.</li>
<li>FP,which is QA .</li>
</ul>
</dd>
</dl>
</li>
<li><a class="reference external" href="https://www.bilibili.com/video/BV1Eb411H7Pq/?spm_id_from=333.788.videocard.0)">B站资源</a></li>
<li><a class="reference external" href="https://looperxx.github.io/CS224n-2019-01-Introduction%20and%20Word%20Vectors/">笔记参考1</a></li>
<li><a class="reference external" href="https://www.cnblogs.com/marsggbo/p/10205943.html">笔记参考2</a></li>
</ul>
</div>
<div class="section" id="word-vectors">
<h2>5.2. 词向量（Word Vectors)<a class="headerlink" href="#word-vectors" title="永久链接至标题">¶</a></h2>
<img alt="../../_images/preface.jpg" src="../../_images/preface.jpg" />
<div class="section" id="id15">
<h3>5.2.1. 自然语言和词义<a class="headerlink" href="#id15" title="永久链接至标题">¶</a></h3>
<ol class="arabic simple">
<li><dl class="first docutils">
<dt>自然语言</dt>
<dd><ul class="first last">
<li>你永远无法确定任何单词对他人意味着什么。（中文这个情况就更普遍啦）</li>
<li>写作是另一件让人类变得强大的事情，这实现了知识的传播和共享。</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>语言的意义</dt>
<dd><ul class="first last">
<li>通过一个词或句子等来表达概念</li>
<li>人们通过文字或声音信号等来表达思想、想法</li>
<li>在写作、艺术中表达含义</li>
</ul>
</dd>
</dl>
</li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>一般通过下面这种语言方式进行有意义的思考:
    signifier(symbol)⇔signified(idea or thing) =denotational semantics
</pre></div>
</div>
<ol class="arabic" start="3">
<li><dl class="first docutils">
<dt>语义计算</dt>
<dd><ul class="first last">
<li><dl class="first docutils">
<dt>常见方案的不足</dt>
<dd><ul class="first last">
<li><dl class="first docutils">
<dt>类似 <a class="reference external" href="https://wordnet.princeton.edu/)">WordNet</a> 一个面向语义的英语词典，包含上义词（hypernyms）、同义词（synonym sets）。</dt>
<dd><ul class="first last simple">
<li>没有考虑上下文，忽略一个词的细微差别</li>
<li>不能及时更新。</li>
<li>Can’t compute accurate word similarity</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>传统NLP的做法。离散符号表示。one-hot，0-1进行编码：Means one 1, the rest 0s</dt>
<dd><ul class="first simple">
<li>向量大小就是词汇表的大小（很多无用的信息）</li>
<li>无法计算相似度。如下例两个词向量是正交的，点积为0.</li>
</ul>
<div class="last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">motel</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">hotel</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>提取新方案</dt>
<dd><ul class="first last simple">
<li>Could try to rely on WordNet’s list of synonyms to get similarity?</li>
<li>learn to encode similarity in the vectors themselves（学习词自身的编码信息）</li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>通过上下文表示词</dt>
<dd><ul class="first last simple">
<li>分布式语义：一个词的含义往往是由附近高频出现的词决定的。</li>
<li>word出现在文本中，这个Word周围会有由词的集合组成的Context出现。这个上下文是固定一个窗口size的。</li>
<li><dl class="first docutils">
<dt>我们可以使用存在Word的大量 <a class="reference external" href="http://ling.cuc.edu.cn/RawPub/)">语料</a> 来学习其向量表示。比如学习“中国科学院”词（实际中会学习每个词），在下列的语料中。</dt>
<dd><ol class="first last arabic">
<li>先向获得2009年度国家最高科学技术奖的 <strong>中国科学院</strong> 院士、复旦大学数学研究所名誉所长谷超豪和</li>
<li>院士、复旦大学数学研究所名誉所长谷超豪和 <strong>中国科学院</strong> 院士、中国航天科技集团公司高级技术顾</li>
<li>大国”向“造船强国”迈进。 由 <strong>中国科学院</strong> 和上海市政府共同建设的上海同步辐射光源工</li>
<li>丽；河南卓越工程管理有限公司董事长邬敏 <strong>中国科学院</strong> 研究生院教授杨佳十人“全国三八红旗手</li>
</ol>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ol>
</div>
<div class="section" id="id17">
<h3>5.2.2. Word Vectors(词向量)<a class="headerlink" href="#id17" title="永久链接至标题">¶</a></h3>
<ul class="simple">
<li>根据一个的词的上下文，来为词构建密集的向量，以使得该向量与出现在类似上下文中的词相似</li>
<li><dl class="first docutils">
<dt>引出词向量，也称词嵌入或词表示。</dt>
<dd><ul class="first last">
<li>word vectors are sometimes called <strong>word embeddings</strong> or word representations.</li>
<li>They are a <strong>distributed representation</strong>.</li>
<li>例如“中国”这个词经过训练后的词向量为：</li>
</ul>
</dd>
</dl>
</li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}中国 = \begin{pmatrix} 0.286\\
            0.792\\
            −0.177\\
            −0.107\\
            0.109\\
            −0.542\\
            0.349\\
            0.271 \end{pmatrix}\end{split}\]</div>
</div>
<div class="section" id="word2vector">
<h3>5.2.3. Word2Vector介绍<a class="headerlink" href="#word2vector" title="永久链接至标题">¶</a></h3>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">Word2vec (Mikolov et al. 2013) 是一种学习词向量的 <em>框架</em>。</p>
</div>
<div class="section" id="id18">
<h4>5.2.3.1. 主要思想<a class="headerlink" href="#id18" title="永久链接至标题">¶</a></h4>
<ol class="arabic simple">
<li>我们有个比较大的文本数据集。</li>
<li>文本中的每个词通过一个固定长度的词向量表示。</li>
<li>扫描文本中每一个位置 <strong>t</strong> 所表示的词,其中有一个中心词 <strong>c</strong> 和上下文词 <strong>o</strong> 。</li>
<li>通过c和o的词向量的相似性，计算在给定c,即中心词来计算o,即上下文的概率。反之亦然。</li>
<li>不断调整词向量来最大化上面提到的概率。</li>
</ol>
<ul>
<li><p class="first">举例如下</p>
<blockquote>
<div><img alt="../../_images/w2v_ex-1.png" src="../../_images/w2v_ex-1.png" />
<img alt="../../_images/w2v_ex-2.png" src="../../_images/w2v_ex-2.png" />
</div></blockquote>
</li>
</ul>
</div>
</div>
<div class="section" id="id19">
<h3>5.2.4. Word2Vector目标函数<a class="headerlink" href="#id19" title="永久链接至标题">¶</a></h3>
<ol class="arabic">
<li><p class="first">思路(后面要说的Skip-grams模型）</p>
<blockquote>
<div><p>在每个位置 <span class="math notranslate nohighlight">\(t\)</span> （t = 1，……，T)，给定一个中心词 <span class="math notranslate nohighlight">\(w_j\)</span> 和一段固定长度的窗口 <span class="math notranslate nohighlight">\(m\)</span>，预测上下文中每个单词的概率。</p>
</div></blockquote>
</li>
</ol>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\begin{split}Likelihood = L(\theta) = \prod_{t=1}^{T}\prod _{\substack{-m\leq j \leq m \\ j\neq 0}}P(w_{t+j}|w_t;\theta)\end{split}\\其中 \theta 是一个需要全局优化的变量\end{aligned}\end{align} \]</div>
<ul>
<li><p class="first">目标函数 <span class="math notranslate nohighlight">\(J(\theta)\)</span> （也称为 <strong>代价或损失函数</strong>）,是一个负对数似然：</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}J(\theta) = -\frac{1}{T}logL(\theta) = -\frac{1}{T}\sum_{t=1}^{T}\sum _{\substack{-m\leq j \leq m \\ j\neq 0}}P(w_{t+j}|w_t;\theta)\end{split}\]</div>
</div></blockquote>
</li>
</ul>
<p>Q1： 如何计算 <span class="math notranslate nohighlight">\(P(w_{t+j}|w_t;\theta)\)</span>?</p>
<p>A1： 每个词w用两个向量表示</p>
<blockquote>
<div><ul class="simple">
<li>当w是中心词时用向量 <span class="math notranslate nohighlight">\(v_w\)</span> 表示</li>
<li>当w是上下文词时用向量 <span class="math notranslate nohighlight">\(u_w\)</span> 表示</li>
</ul>
</div></blockquote>
<p>那么对于一个中心词c和上下文词o可用如下形式表示</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[P(o|c) = \frac{exp(u_o^T v_c)}{\sum_{w\in V} exp(u_w^Tv_c)}\]</div>
<p>其中，<span class="math notranslate nohighlight">\(u_o^T v_c`点积表示o和c的相似度，点积越大则表示概率越大；:math:`{\sum exp(u_w^Tv_c)}\)</span>
目的是为了对整个词汇表进行标准化。</p>
</div></blockquote>
<ul>
<li><dl class="first docutils">
<dt>softmax function</dt>
<dd><p class="first">softmax函数作用是将任意标量 <span class="math notranslate nohighlight">\(x_i`映射为概率分布 :math:`p_i\)</span> 。</p>
<div class="math notranslate nohighlight">
\[softmax(x_i) = \frac{exp(x_i)}{\sum_{j=1}^{n} exp(u_w^Tv_c)} = p_i\]</div>
<ul class="last simple">
<li>“max”对比较大的 <span class="math notranslate nohighlight">\(x_i\)</span> 映射比较大的概率</li>
<li>”soft” 对那些小的 <span class="math notranslate nohighlight">\(x_i\)</span> 也会给予一定概率</li>
<li>这是一种常见的操作，如深度学习</li>
</ul>
</dd>
</dl>
</li>
</ul>
<div class="admonition tip">
<p class="first admonition-title">小技巧</p>
<ul class="last simple">
<li>利用对数的特性将目标函数转换为对数求和，减少计算的复杂度。</li>
<li>最小化目标函数 ⟺最大化预测的准确率</li>
</ul>
</div>
<ul class="simple">
<li>通过不断的优化参数最小化误差来训练模型。</li>
<li><dl class="first docutils">
<dt>为了训练模型，需要计算所有向量的梯度</dt>
<dd><ul class="first last">
<li><span class="math notranslate nohighlight">\(\theta\)</span> 用一个很长的向量表示所有模型的参数。</li>
<li><dl class="first docutils">
<dt>每个单词有个两个向量。</dt>
<dd><ul class="first last">
<li>Why two vectors? àEasier optimization. Average both at the end.</li>
</ul>
</dd>
</dl>
</li>
<li>利用不断移动的梯度来优化这些模型的参数。</li>
</ul>
</dd>
</dl>
</li>
</ul>
</div>
<div class="section" id="id20">
<h3>5.2.5. 梯度计算推导<a class="headerlink" href="#id20" title="永久链接至标题">¶</a></h3>
<p>下面开始推导 <span class="math notranslate nohighlight">\(P(w_{t+j}|w_t;\theta)\)</span>:
对 <span class="math notranslate nohighlight">\(v_c\)</span> 求偏微分</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\begin{split}\frac{\partial}{\partial v_c }logP(o|c)
&amp;= \frac{\partial}{\partial v_c }log\frac{exp(u_o^T v_c)}{\sum_{w\in V} exp(u_w^Tv_c)}\\
&amp;=\frac{\partial}{\partial v_c}\left(\log \exp(u_o^Tv_c)-\log{\sum_{w\in V}\exp(u_w^Tv_c)}\right)\\
&amp;=\frac{\partial}{\partial v_c}\left(u_o^Tv_c-\log{\sum_{w\in V}\exp(u_w^Tv_c)}\right)\\
&amp;=u_o-\frac{\sum_{w\in V}\exp(u_w^Tv_c)u_w}{\sum_{w\in V}\exp(u_w^Tv_c)}\\
&amp;=u_o-\sum_{w\in V}\frac{\exp(u_w^Tv_c)}{\sum_{w\in V}\exp(u_w^Tv_c)}u_w\\
&amp;=u_o-\sum_{w\in V}P(w|c)u_w\\\end{split}\\\begin{split}其中，u_o是我们观测到每个词的值，\sum_{w\in V}P(w|c)u_w是模型的预测值，\\
利用梯度下降不断使两者更为接近，使偏微为0.\end{split}\end{aligned}\end{align} \]</div>
<p>还有对 <span class="math notranslate nohighlight">\(u_o\)</span> 的偏微过程，大家动手推导下，比较简单的。</p>
<div class="admonition tip">
<p class="first admonition-title">小技巧</p>
<p>补充一点边角知识，在上面的推导过程中用的到：</p>
<ul class="last simple">
<li>向量函数与其导数 <span class="math notranslate nohighlight">\(\frac{\partial Ax}{\partial x} = A^T, \frac{\partial x^T A}{\partial x} = A\)</span></li>
<li>链式法则：<span class="math notranslate nohighlight">\(log'f[g(x)] = \frac{1}{f[g(x)]}g'(x)\)</span></li>
</ul>
</div>
</div>
<div class="section" id="id21">
<h3>5.2.6. word2vector的概览<a class="headerlink" href="#id21" title="永久链接至标题">¶</a></h3>
<p>前面提到的Word2Vector是一种学习词向量的框架（模型），它包含两个实现算法：</p>
<ol class="arabic">
<li><p class="first">Skip-grams (SG) （课上讲的就类似这种）</p>
<blockquote>
<div><ul class="simple">
<li>根据中心词周围的上下文单词来预测该词的词向量</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">Continuous Bag of Words (CBOW)</p>
<blockquote>
<div><ul class="simple">
<li>根据中心词预测周围上下文的词的概率分布。</li>
</ul>
</div></blockquote>
</li>
</ol>
<p>另外提到两个训练的方式：</p>
<ol class="arabic">
<li><p class="first">negative sampling (比较简单的方式)</p>
<blockquote>
<div><ul class="simple">
<li>通过抽取负样本来定义目标</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">hierarchical softmax</p>
<blockquote>
<div><ul class="simple">
<li>通过使用一个树来计算所有词的概率来定义目标。</li>
</ul>
</div></blockquote>
</li>
</ol>
</div>
<div class="section" id="id22">
<h3>5.2.7. 优化：梯度下降与随机梯度下降算法的要点<a class="headerlink" href="#id22" title="永久链接至标题">¶</a></h3>
<ul>
<li><dl class="first docutils">
<dt>梯度下降（Gradient Descent，GD）</dt>
<dd><ol class="first arabic simple">
<li>最小化的目标（代价）函数 <span class="math notranslate nohighlight">\(J(\theta）\)</span></li>
<li>使用梯度下降算法去优化 <span class="math notranslate nohighlight">\(J(\theta）\)</span></li>
<li>对于当前 <span class="math notranslate nohighlight">\(\theta\)</span> 采用一个合适的步长（学习率）不断重复计算 <span class="math notranslate nohighlight">\(J(\theta）\)</span> 的梯度，朝着负向梯度方向。</li>
</ol>
<img alt="../../_images/SG.jpg" src="../../_images/SG.jpg" />
<ol class="last arabic" start="4">
<li><p class="first">更新等式（矩阵）</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\theta^{new} = \theta^{old} - \alpha\nabla_\theta J(\theta)\\其中，\theta = 步长（学习率）\end{aligned}\end{align} \]</div>
</div></blockquote>
</li>
<li><dl class="first docutils">
<dt>更新等式（单个参数）</dt>
<dd><div class="first last math notranslate nohighlight">
\[\theta_j^{new} = \theta_j^{old} - \alpha\frac{\partial}{\partial \theta_j^{old}}J(\theta)\]</div>
</dd>
</dl>
</li>
</ol>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>随机梯度下降（Stochastic Gradient Descen, SGD）</dt>
<dd><ul class="first last simple">
<li><dl class="first docutils">
<dt>目的</dt>
<dd>进一步解决 <span class="math notranslate nohighlight">\(J(\theta）\)</span> 的训练效率（因为目标函数包含所有的参数，而且数据集一般都是很大的）问题：太慢了。</dd>
</dl>
</li>
<li>Repeatedly sample windows, and update after each one</li>
</ul>
</dd>
</dl>
</li>
</ul>
</div>
<div class="section" id="id23">
<h3>5.2.8. 小结<a class="headerlink" href="#id23" title="永久链接至标题">¶</a></h3>
<ul class="simple">
<li>本节首先从语言的语义问题开始讲起，然后为了表示语义，引出了词向量概念，接着着重讲了Word2Vector框架、原理、算法推导等，最后简单提了下目标函数的优化的方式。</li>
<li><dl class="first docutils">
<dt>看完并梳理完本节知识，我产生了几个问题：</dt>
<dd><ul class="first last">
<li>词向量提了好多次，那么每个词的词向量究竟是如何产生（计算）的呢？存在哪些方法？</li>
<li><dl class="first docutils">
<dt>有几个点的原理还需进一步深入理解：</dt>
<dd><ul class="first last">
<li>负采样、层次采样；区别和前后的优势在哪里？</li>
<li>SG、CBOW算法的细节；本质区别和各自优势是什么？</li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
</div>
</div>
<div class="section" id="id24">
<h2>5.3. 词向量和语义<a class="headerlink" href="#id24" title="永久链接至标题">¶</a></h2>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">本节课后，我们就能够开始读一些关于词嵌入方面的论文了。</p>
</div>
<img alt="../../_images/preface-2.png" src="../../_images/preface-2.png" />
<div class="section" id="review-word2vec">
<h3>5.3.1. Review：word2vec<a class="headerlink" href="#review-word2vec" title="永久链接至标题">¶</a></h3>
<div class="section" id="id25">
<h4>5.3.1.1. 主要思想<a class="headerlink" href="#id25" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li><dl class="first docutils">
<dt>这里以skip-gram）模型为例</dt>
<dd><ol class="first last arabic">
<li>遍历整个语料库的每个词，通过中心词向量预测周围的词向量</li>
<li>算法学到的词向量能用来计算词的相似度或语义等相关需求。</li>
</ol>
</dd>
</dl>
</li>
</ul>
</div>
<div class="section" id="id26">
<h4>5.3.1.2. 关于梯度计算<a class="headerlink" href="#id26" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li>GD。计算效率低，每次对所有样本进行梯度计算</li>
<li>SGD。每次只对一个固定大小的样本窗口进行更新，效率较高。</li>
<li><dl class="first docutils">
<dt>梯度计算存在稀疏性（0比较多）</dt>
<dd><ul class="first last">
<li>But in each window, we only have at most 2m + 1 words, so it is very sparse!</li>
<li><dl class="first docutils">
<dt>解决方案：</dt>
<dd><ul class="first last">
<li>only update certain rows of full embedding matrices U and V. (使用稀疏矩阵仅更新稀疏性低的词向量矩阵U和V)</li>
<li>you need to keep around a hash for word vectors （使用hash来更新，即k-v，k表示word,v表示其词向量）</li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
</div>
<div class="section" id="negative-sample">
<h4>5.3.1.3. 基于负采样(negative sample)方法计算<a class="headerlink" href="#negative-sample" title="永久链接至标题">¶</a></h4>
<ol class="arabic">
<li><dl class="first docutils">
<dt>计算下列式子：</dt>
<dd><div class="first math notranslate nohighlight">
\[P(o|c) = \frac{exp(u_o^T v_c)}{\sum_{w\in V} exp(u_w^Tv_c)}\]</div>
<p class="last">其中 :math:` {sum_{win V} exp(u_w^Tv_c)}` 计算代价非常大（整个语料库计算），如何降低这一块的计算复杂度就是需要考虑的问题。</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>负采样方法介绍</dt>
<dd><ul class="first last simple">
<li>主要思想：train binary logistic regressions。除了对中心词窗口大小附近的上下文词取样以外(即true pairs)，还会随机抽取一些噪声和中心词配对（即noise pairs）进行计算，而不是遍历整个词库。</li>
<li>这个 <strong>负</strong> 指的是噪声数据（无关的语料词 noise pairs）</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">负采样计算细节</p>
</li>
</ol>
<div class="admonition tip">
<p class="first admonition-title">小技巧</p>
<p class="last">可参考论文 <a class="reference external" href="https://github.com/lihanghang/NLP-Knowledge-Graph/raw/master/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%AF%AD%E8%A8%80%E8%A1%A8%E7%A4%BA%E6%A8%A1%E5%9E%8B/Tomas%20Mikolov%20papers/Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality.pdf">Distributed Representations of Words and Phrases and their Compositionality” (Mikolov et al. 2013)</a> 第3页。</p>
</div>
<ul>
<li><p class="first">最大化下面的目标函数</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[J(\theta) = \frac{1}{T}\sum_{t=1}^{T}J_{t}(\theta)\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}J_{t}(\theta)=\log \sigma(u_{o}^{T} v_{c})+\sum_{i=1}^{k} \mathbb{E}_{j \sim P(w)}[\log \sigma(-u_{j}^{T} v_{c})]\\其中，\sigma(x)=\frac{1}{1+e^{-x}}\end{aligned}\end{align} \]</div>
<ul class="simple">
<li>公式第一项表示最大化真实的中心词和其上下文词的概率；第二项是最小化负采样的噪声值（中心词及其上下文）的概率，j表示负采样的样本，并以P(w)大小进行随机采样。</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<ul class="last simple">
<li>P(w)，这里使用了N元统计模型且N取1，即一元统计模型（unigram），表示每个词都和其它词独立，和它的上下文无关。每个位置上的词都是从多项分布独立生成的。</li>
<li>补充N元统计模型，N=2时就为二元统计模型，即每个词和其前1个词有关。一般的，假设每个词 <span class="math notranslate nohighlight">\(x_t\)</span> 只依赖于其前面的n−1个词（n 阶马尔可夫性质）。</li>
<li>通过N元统计模型，我们可以计算一个序列的概率，从而判断该序列是否符合自然语言的语法和语义规则。</li>
<li>这个方法在构建词向量时最大的问题就是 <strong>数据稀疏性</strong>，大家可以思考下为什么？还能想到改进或其他更好的方法？</li>
</ul>
</div>
</div></blockquote>
</li>
</ul>
</div>
</div>
<div class="section" id="id27">
<h3>5.3.2. 基于共现矩阵生成词向量<a class="headerlink" href="#id27" title="永久链接至标题">¶</a></h3>
</div>
<div class="section" id="glove">
<h3>5.3.3. Glove词向量模型<a class="headerlink" href="#glove" title="永久链接至标题">¶</a></h3>
</div>
<div class="section" id="id28">
<h3>5.3.4. 词向量评估<a class="headerlink" href="#id28" title="永久链接至标题">¶</a></h3>
</div>
<div class="section" id="id29">
<h3>5.3.5. 语义<a class="headerlink" href="#id29" title="永久链接至标题">¶</a></h3>
</div>
<div class="section" id="id30">
<h3>5.3.6. 分类和神经网络的区别<a class="headerlink" href="#id30" title="永久链接至标题">¶</a></h3>
</div>
<div class="section" id="id31">
<h3>5.3.7. 建议<a class="headerlink" href="#id31" title="永久链接至标题">¶</a></h3>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="kg.html" class="btn btn-neutral float-right" title="6. 知识图谱" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="dl.html" class="btn btn-neutral float-left" title="3. 深度学习" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, LiHangHang

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>